\chapter{Policy Implications}
\label{chap:policy}

The work presented in this document impinges a number of policy
questions in the security and privacy space. A number of these
implications have been alluded to in the previous chapters. This
chapter explores some of these policy questions in greater depth.

\section{Toward a Healthy SSaaS Ecosystem}

If the Secret Storage as a Service (SSaaS) ecosystem is to flourish,
the public must first be convinced that it is a viable solution for
increasing the security and privacy of the devices and services they
use (and that it does not have a detrimental impact on their ability
to utilize such devices and services). While policy initiatives can do
little to convince users that the SSaaS ecosystem does not impose an
undue burden on their desired use cases\footnote{That's the job of
  previous chapters in this document, as well as of developers and
  marketers building SSaaS-backed solutions.}, policy initiatives can
help to encourage the development of a security and privacy preserving
SSaaS ecosystem. Achieving such an outcome relies on three main
components:

\begin{packed_desc}
\item[Validity of the Secret Storage System:] Users must have some
  degree of faith in the security of the underlying SSaaS technologies
  and protocols. If such technology is viewed with suspicion, the
  SSaaS ecosystem is unlikely to thrive.
\item[Validity of Secret Storage Providers:] Users must also have some
  degree of faith in the security of individual Secret Storage
  Providers (SSPs) and the privacy of data stored there. While a user
  can take steps to minimize how much they must trust each SSP (see
  next point), if SSPs are viewed as unreliable and untrustworthy in
  general, the SSaaS ecosystem will suffer.
\item[Ability to Minimize Trust:] While a degree of trust on both the
  SSaaS protocols as well as in each SSP is useful to help the SSaaS
  ecosystem thrive, it's also important that users have tools for
  minimizing the degree to which they must trust any single part of or
  party in the SSaaS system. This ability acts as the final security
  and privacy backstop for users, ensuring that their data remains
  secure even if their trust in specific protocols or SSPs turns out
  to be misplaced.
\end{packed_desc}

There are a number of policy initiatives that will help to encourage
the development of these user views, and by proxy, the growth of a
health SSaaS ecosystem. These initiatives include:

\begin{packed_desc}
\item[Access to Strong Cryptography:] In order to ensure the security
  of the underlying technology and protocols, it is critical that user
  are allowed to access and utilize strong cryptography.
\item[SSP Privacy and Security:] In order to instill (at least
  minimal) trust in individual SSPs, its important that users have an
  ``expectation of privacy'' for the data they store with each
  SSP. Furthermore, it is important for users to have the right to
  seek relief from SSPs who's practices result in the exposure of user
  secrets.
\item[Openness and Transparency:] One way of minimizing the trust the
  user must place in the SSaaS ecosystem is to insure that the core
  SSaaS software and protocols remain open and auditable. Such an
  openness can be achieved by making all core code open source and by
  applying liberal licensing to all core protocols .
\item[Use of Multiple SSPs:] As discussed, the main mechanism for
  limiting trust in any single SSP is to shard secrets across multiple
  SSPs. As such, it is important to both avoid polices that limit the
  ability of user to leverage a wide range of SSPs while also
  leveraging policies that encourage the development of an multitude
  of competing secret storage providers.
\end{packed_desc}

Each of these policy goals, as well as specific ideas for potential
achievement of each goal, are discussed through the reminder of this
chapter.

\section{Access to Strong Cryptography}
\label{chap:policy:crypto}

Many of the ideas proposed in this document rely on the use of a
variety of cryptographic primitives, including symmetric encryption,
message authentication codes (MAC), asymmetric encryption, asymmetric
authentication, and cryptographic signatures. Indeed, the security of
the SSaaS ecosystem rests on the ability of users to leverage a range
of cryptographic primitives. Such cryptographic primitives, however,
have a checkered legal and policy history, at least in the United
States\footnote{the bulk of this chapter will focus on US policy and
  laws since that is the jurisdiction in which the author resides and
  is most intimately familiar. Similar ideas are applicable to other
  jurisdictions.}. This section provides an overview of
cryptography-related policy concerns.

\subsection{A Brief History of Cryptography Regulation}

Strong cryptography is a relatively recent development in the timeline
of human history. Modern digital cryptography didn't really come about
until the end World War II and the advent of information
theory~\cite{shannon1945}. Prior to that point in time, most
``cryptography'' systems were based on folk-theory, obscure languages,
one-time pads, mechanical devices, or other items not soundly rooted
in mathematical theory. Even though the groundwork was laid by the end
of World War II, modern practical digital cryptography systems didn't
really become available until the publication of DES in the mid
1970s~\cite{fips46} and the invention of asymmetric cryptography
around the same time~\cite{diffie1976}. The combination of a
standardized symmetric key algorithm coupled with the ability to
negotiate and/or exchange symmetric keys over insecure channels using
asymmetric techniques made digital cryptography a practical tool for
securing communications. Unlike previous systems, the security of such
communications rested on the provable difficulty of solving certain
classes of mathematical problems. From the late 1970s through the
1980s, digital cryptography remained largely a tool for governments,
militaries, and financial corporations, and as such was often
classified as a ``munition'' and placed under various export control
laws regulating the manner in which encryption technology could be
distributed outside of the United States.\footnote{The U.S. did allow
  the export of certain types of purposely weak crypto, e.g. crypto
  with key lengths below 40 bits. This had the extremely unfortunate
  effect of encouraging the standardization and proliferation of
  various weak crypto algorithms in a wide range of consumer-facing
  software in order to allow the international distribution of such
  software. The proliferation of such ``export-grade'' cryptography
  still comes back to bite us today -- many applications still contain
  support for such algorithms, and that support makes those products
  vulnerable to ``downgrade'' attacks that trick the software into
  using this weak crypto. Recent vulnerabilities such as
  FREAK~\cite{beurdouche2015} and Logjam~\cite{adrian2015} are based
  on such attacks.} During this time, cryptographically secure systems
were not readily available for use by individuals not associated with
the aforementioned institutions.

The availability of strong cryptography to the general public begin to
increase in the early 1990s with the release of
PGP~\cite{zimmermann-pgp10}. PGP quickly became the first widely
distributed encryption software designed for use by ordinary
individuals.\footnote{Although, as mentioned previously, the ability
  of ``ordinary'' individuals to actually use PGP is highly
  questionable\cite{whitten1999}.}  PGP's Internet-based distribution
also quickly drew the attention of US authorities who viewed such
publication as a violation of the export control laws banning the
export of cryptographic technologies with key lengths exceeding
40-bits. In defiance of such accusations, PGP's author published the
entire source code of PGP as a traditional bound book, the contents of
which could be scanned and compiled by anyone with a printed
copy~\cite{zimmermann-pgpsource}. This publication forced a discussion
of the First Amendment~\cite{us-constitution-amend1} issues associated
with the distribution of computer source code. Such discussions,
coupled with several court cases~\cite{ninthcir-bernstein,
  sixthcir-junger} in favor of the protection of the distribution
computer source code under the First Amendment, led to the US
government rolling back (although not completely eliminating) most of
the export control rules surrounding cryptography by the early
2000s~\cite{kehl2015}.

During this same period, the government tried to standardize a
backdoored encryption system known as the Clipper
Chip~\cite{whitehouse-clipper}. The chip was designed to be embedded
in systems where it would provide ``strong'' encryption via the
Slipjack algorithm while maintaining the government's ability to
decrypt such encryption via the use of escrowed master keys. The
Clipper Chip quickly feel into disfavor, however, when researchers
demonstrated simple methods for bypassing its escrow
mechanisms~\cite{blaze1994}. The publication of such mechanisms was
soon followed by discoveries of weaknesses in the Slipjack
algorithm~\cite{biham1998}. Due to these flaws, and a general distrust
of any system requiring the escrowing of encryption keys with the
government, the Clipper chip was never widely adopted and the
government largely gave up on pushing backdoored encryption systems by
the start of the 2000s.

After the failure of both the government's export control based
cryptography regulations and their push for key escrow-based
encryption standards, efforts to control and regulate cryptography
were largely quite through the 2000s and early 2010s. Recently,
however, this has begun to change. The Edward Snowden revelations
related to spying abuses by the US National Security Agency (NSA) in
the early 2010s led to a rise in privacy awareness among the general
populace~\cite{pew-privsec14}. This, in turn, led a push to increase
the use of strong cryptography across a wide range of
consumer-oriented products, from websites~\cite{mozilla-deprecatehttp}
to email~\cite{gmail-blog-encryption} to
smartphones~\cite{ars-ios-encrypt, ars-android-encrypt}. The
associated increase in end-user use of cryptography has led to renewed
calls by law enforcement agencies around the world (including the US
FBI~\cite{comey-testimony-encryption}) to mandate the use of breakable
encryption in any scenario were a government feels they are justified
in accessing the underlying data (e.g. when issues a probably cause
warrant). Such calls have culminated in the ongoing court battle
between Apple and FBI over whether or not the FBI can use the All
Writs Act~\cite{usc-allwrits} to compel Apple to modify the iPhone
operating system to expedite the rate at which the FBI can guess the
pass-code required to decrypt a phone~\cite{ars-cookvfbi}. Congress
has also responded to such calls by suggesting legislation ranging
from the formation of encryption ``study'' committees~\cite{hr4651} to
outright bans on the use of strong encryption~\cite{bennett-burrbill}
to bans on such bans of encryption~\cite{hr4528}. How these court
cases and legislative actions will play out remains to be seen.

\subsection{A Defense of Cryptography}

From the export-control bans of the first crypto wars~\cite{kehl2015}
to the Apple v FBI crypto debate today, whether or not strong
cryptography should be available to the general public is a hotly
contested topic. Should the government ever succeed in banning or
strongly curtailing the development, distribution, or use of strong
cryptography, it will become substantially more difficult (if not
impossible) to securely implement SSaaS-related ideas, and by proxy,
the privacy and security enhancements SSaaS systems might
provide. Indeed, any effort that curtails the use of or access to
strong cryptography is likely to damage the security of a wide range
of digital systems.

Government efforts to ban cryptography have been countered with a
variety of policy and legal arguments. These arguments won out during
the efforts to regulate and ban cryptography during the
1990s. Hopefully, they will win out again today.

From a policy perspective, there are a variety arguments against
efforts to ban or regulate various form of cryptography. They tend to
fall along a three standard tracks:

\begin{packed_desc}
\item[More Harm Than Good] \hfill \\ As discussed in this document and
  elsewhere~\cite{abelson2015}, cryptography is a critical tool for
  protecting the security and privacy of modern computing
  systems. Thus, any effort aimed at weakening cryptography for the
  purpose of minimizing its effectiveness to ``bad'' actors is just as
  likely to minimizing its effectiveness to ``good'' actors. And many
  ``good'' actor rely on cryptography to protect their systems and
  data. Efforts to curtail the use of strong cryptography would
  severely damage the security of financial, communication, and data
  storage systems. Any backdoor available to the US government is an
  additional point of failure that an attacker may use to compromise
  and access a system. Furthermore, any policy of providing the US
  government with special access to cryptographically protected data
  will likely be mirrored by other countries -- many of which may have
  far weaker legal protections against the abuse of such
  powers. E.g. if the US government can demand special access, so can
  China or North Korea -- weakening the security of US citizens abroad
  or anyone wishing to secure their selves and systems from
  potentially repressive regimes. Bans of cryptography thus trade the
  security of a large number of normal individuals for the insecurity
  of a small number of potentially bad actors -- and that's not a
  trade that's worth making.
\item[Ineffectiveness] \hfill \\ Beyond the fact that bans on
  cryptography are likely to cause more harm than good, such bans are
  widely believed to be ineffective. Even if the US manages to ban the
  use of cryptography, cryptographic concepts are no longer a niche
  subject. There are hundreds cryptographic products available
  worldwide, more than half of which are developed and distributed
  outside of the Untied States~\cite{schneier2016}. The Internet and
  modern commerce systems make such products widely available to
  almost anyone. Thus, even if the US manages to ban cryptography,
  those wishing to use it will just obtain cryptography-related
  products elsewhere. Modern cryptographic concepts are also widely
  understood and taught~\cite{schneier2010crypto}. There are thus a
  wide variety of individuals capable of creating cryptographically
  secure systems from scratch.\footnote{Although it is not advisable
    for most individuals to attempt to ``roll their own'' cryptography
    if they can avoid needing to do so. Such DIY low-level
    cryptographic systems have a historically high likelihood of
    mistakes due to the lack of formal vetting and widespread review.}
  Thus, it would not be difficult for organizations or individuals
  wishing to employ banned cryptography concepts to find someone
  to build such systems for them.
\item[Better Options] \hfill \\ Law enforcement organizations (LEOs)
  often cite the ``going dark'' problem as the reason why
  cryptographic restrictions are necessary. ``Going dark'' refers to
  the idea that as the use of cryptography grows the ability of law
  enforcement to monitor criminal activity for the purpose of
  preventing or prosecuting crimes
  diminishes~\cite{anderson2013}. What this viewpoint fails to
  acknowledge is the fact that this is the golden age of surveillance:
  law enforcement agencies have access to more data than they ever
  have had before~\cite{swire2011}. Thus, even with the widespread use
  of encryption, LEOs have more access to data about a potential
  criminal than at any prior point in human history. As such, there
  are a multitude of ways to combat crime without weakening
  cryptography. Such mechanisms may require more time and effort than
  the kind automated digital surveillance the use of cryptography
  subverts, but such time and effort is a necessary price to pay to
  ensure a proper balance between privacy and security. Instead of
  weakening encryption, law enforcement can focus their efforts on
  targeted investigations of specific individuals, leveraging the
  large digital footprints even cryptography-employing users leave
  behind. And even when using cryptography, individuals are still
  prone to a wide range of targeted attacks, from efforts to
  compromise individual systems, and subvert any cryptography they may
  employ, to social engineering attacks that tend to be very effective
  against even cryptography employing adversaries. There are numerous
  tools available to law enforcement to pursue criminals. Weakening
  encryption need not be one of them.
\end{packed_desc}

Beyond policy arguments against regulating or banning cryptography,
there are also a number of legal arguments preventing such efforts. In
particular, any effort to ban cryptography would raise serious
constitutional questions, including:

\begin{packed_desc}
\item[1\textsuperscript{st} Amendment] \hfill \\ As previously
  mentioned, bans on the publication of source code,
  cryptography-related or otherwise, raise numerous
  1\textsuperscript{st} Amendment questions. The free speech
  guarantees provided by the 1\textsuperscript{st} Amendment have
  generally been interpreted to apply to source
  code~\cite{ninthcir-bernstein, sixthcir-junger}.\footnote{Indeed,
    this ``code as speech'' interpretation is fairly critical to the
    use of US Copyright Law for the protection of code-related
    intellectual property.} As such, any ban on cryptography source
  code is likely an unconstitutional prior restraint on the author's
  freedom of speech. Traditionally, courts have taken a dim few of
  laws imposing such prior restraint~\cite{scotus-nearvminnesota},
  even in cases where national security my be
  impinged~\cite{scotus-nytvus}. Additionally, efforts to mandate the
  inclusion of a backdoor in certain software are likely a form of
  unconstitutional compelled speech. Just as blocking the publication
  of source code interferes with the author's free speech rights,
  mandating an author publishes or includes certain components in
  their code is also a violation of their free speech
  rights~\cite{scotus-wooleyvmaynard}. While there have been
  exceptions to such interpretations, it is unlikely the broadness of
  any general limits of cryptography, coupled with the traditional
  ineffectiveness of such limits, would meet the kind of ``narrowly
  tailored speech limits in effective support of a legitimate
  government interest'' requirements such a law would have to overcome
  to meet constitutional muster. Still open to debate, however, is
  whether or not first amendment protections are limited merely to the
  source code implementing encryption, or whether they apply to
  executable binaries built form such code as well. If such
  protections are limited to raw source code, it could effectively
  prevent the widespread use of cryptography to those capable of
  acquiring and building source code. Such an outcome would have a
  detrimental impact on the general populace who often lack the
  ability to perform such actions.
\item[2\textsuperscript{nd} Amendment] \hfill \\ One of the more
  creative constitutional defenses of cryptography~\cite{xkcd-504}
  comes via the 2\textsuperscript{nd} Amendment granting citizens the
  right to ``keep and bear arms''~\cite{us-constitution-amend2}. The
  idea is that since the government already has a history of
  considering cryptography to be a weapon, then the second amendment
  should be used to protect the rights of citizens to posses and use
  cryptography. This theory has never been tested in a court, but it
  is not entirely unreasonable to believe that cryptography is a
  necessary component of maintaining a ``well regulated militia''
  today, and thus should be protected by the 2\textsuperscript{nd}
  Amendment~\cite{scotus-usvmiller-guns}. It is also not unreasonable
  to assume that if the Constitution were being written today,
  cryptography would seem a much more natural fit for protection via
  something like the second amendment then firearms. Whether or not
  this argument will pass judicial scrutiny remains to be seen. And
  even if it does, the 2\textsuperscript{nd} Amendment is likely to
  guarantee a weaker set of rights than 1\textsuperscript{st}
  Amendment protections since the 2\textsuperscript{nd} Amendment only
  protects the right to ``keep and bear'' cryptography, where as the
  1\textsuperscript{st} Amendment also protects the right to
  distribute cryptography. It may be possible, however, to take a
  split approach to protecting cryptography where cryptographic source
  code and research are protected via the 1\textsuperscript{st}
  Amendment while binary programs or hardware employing cryptography
  are protected via the 2\textsuperscript{nd} Amendment -- similar in
  mechanism to how the 1\textsuperscript{st} Amendment would protect
  the publication of a design of a firearm while the
  2\textsuperscript{nd} Amendment would protect the possession of the
  firearm itself.
\item[4\textsuperscript{th} Amendment] \hfill \\ Many law enforcement
  agencies cite the importance of being able to access information for
  which they have secured probable cause warrant as their reason for
  wishing to restrict the use of cryptography. Such opponents fear the
  ability of cryptography to render such lawfully obtained warrants
  useless. Whether or not the 4\textsuperscript{th}
  Amendment~\cite{us-constitution-amend4} can be used to compel a
  service provider to decrypt data in order to comply with a valid
  warrant remains to be seen. Traditional interpretations (including
  the much maligned Third Party Doctrine~\cite{thompson-thirdparty})
  hold that if a third party holds the key necessary to decrypt a
  user's data (or the data itself), then they can be compelled to
  provide it to the government -- often without requiring a probable
  cause warrant at all. But what about cases where the third party
  lacks any ability to decrypt the data? Can they still be forced to
  aid the government in serving a valid warrant?  The answer to this
  question probably hinges on the ``reasonableness'' of such a
  request. The ongoing Apple v. FBI litigation may help shed some
  light on this question. At its heart, it would seem that
  cryptography provides the ultimate tool for protecting an
  individual's 4\textsuperscript{th} Amendment rights - since such
  technology helps ensure that individuals remain ``secure in their
  persons, houses, papers, and effects''. But how best to reconcile
  this protection with a valid ``probable cause'' warrant remains a
  matter of debate. And ultimately, as in cases where only the end
  user posses the information necessary to decrypt communication, the
  answer to this question may have more bearing on the
  5\textsuperscript{th} Amendment than the 4\textsuperscript{th}.  It
  also seems unlikely that the need to serve a 4\textsuperscript{th}
  Amendment warrant would ever justify the potential violations of the
  1\textsuperscript{st} and 2\textsuperscript{nd} Amendments that any
  general ban on the use of cryptography would entail.
\item[5\textsuperscript{th} Amendment] \hfill \\ Assuming the
  1\textsuperscript{st} and 2\textsuperscript{nd} Amendments can
  protects an individuals right to distribute, posses, and use
  cryptography, then it falls on the 5\textsuperscript{th} Amendment
  to protect an individuals right not to be forced to disclose the
  necessary password or encryption key required to decrypt a piece of
  information. The right against self incrimination has long been held
  to protect individuals against compelled testimony -- which should
  include being forced to provide a memorized pass-code or password
  necessary to decrypt a piece of information.\footnote{If the user
    has written down their password or locked their phone with a
    fingerprint, 5\textsuperscript{th} Amendment protections no longer
    apply since the evidence is then physical, not testimonial.}
  Courts, however, are currently split on whether or not the
  5\textsuperscript{th} Amendment protects individuals from being
  forced to provide decryption assistance~\cite{usvboucher,
    commonwealthvgelfgatt, usvdoe}. Further complicating matters,
  certain types of cryptography can be designed to decrypt a
  ciphertext to different plaintexts depending on the password or key
  provided. Thus, it is possible that an individual could provide one
  password to decrypt a file to a picture of a cat and another
  password to decrypt a file to a plan for robbing a bank. Additional
  passwords could provide additional alternative contents. In such an
  arrangement, there is no reliable way to know how many different
  plaintexts are hidden within a given ciphertext. Thus, even if the
  government does succeed in compelling an individual to ``decrypt''
  their documents, it is entirely possible they would still wind up
  with a set of files other than the ones they were seeking. And it
  seems unlikely any court would allow the government to continue
  compelling a suspect to provide new pass-phrases until they happened
  to get a decrypted set of files that favored their case, especially
  since it is possible no such files exist at all. The
  5\textsuperscript{th} Amendment has historically protected the
  contents of an individuals mind. Cryptography allows an individual
  to expand the contents of their mind by leveraging a small piece of
  memorized information (i.e. a pass-code or praise) to protect much
  larger pieces of offloaded information (e.g. a hard drive). Whether
  or not such an expansion is accessible to the US legal system
  remains to be decided.\footnote{Deciding whether or not such
    expansions are acceptable using cryptography will likely have a
    direct implication on the potential future world where human
    brains are technologically upgradeable. Indeed, there is likely
    little functional difference between using a cryptographically
    protected hard drive coupled with a memorized pass-code to expand
    one's ``memory'' and installing a brain-linked computer chip that
    expands one's memory. In a world were you have the technology to
    upgrade the the amount of data your mind can store (and
    potentially where the government has the technology to
    ``mind-read'' by downloading such data), does the
    5\textsuperscript{th} Amendment still protect the contents of
    one's mind? While such questions remain largely academic today, it
    may not be long before courts are being asked to rule on them.}
\end{packed_desc}

There are clear policy reasons to avoid curtailing or discouraging the
use of cryptography, as well as clear legal hurtles to doing so. It is
critical that cryptography remains widely and freely available to all,
since any reduction in access to cryptography would have severe
consequence on a range of security enhancing and privacy preserving
technologies. This included on the SSaaS ideas proposed in this
document.

% Maximizing Trustworthiness
\section{Privacy of SSP-Stored Data}

Secret Storage providers (SSPs) are likely to be appealing targets for
both legal and extralegal introspection. Whether such services are
storing encryption keys for user data or raw user secrets, they will
be the target of a range of government information requests, criminal
hacking attempts, and general malfeasance. This fact suggests a number
of policy-related questions and potential solutions. As stated
previously, in order for the SSaaS ecosystem to flourish, users must
enjoy at least a modicum of privacy protections for the secrets they
store with the SSP. What policy initiatives are necessary to maximize
the privacy of SSP-stored data?

\subsection{Third Party Privacy}

SSP servers are most likely to be operated by third parties (as
opposed to operated by the individual requiring secret storage
services). As such, they are likely to be subject to the third party
doctrine~\cite{thompson-thirdparty}. This doctrine holds that
individuals who voluntarily store their data with third parties have
no reasonable expectation of privacy~\cite{scotus-katzvus} for such
data. While such a viewpoint may have made since in the mid 20th
century when it was established via a series of Supreme Court
rulings~\cite{scotus-usvmiller-privacy, scotus-smithvmaryland}, it
does not translate well to a world where storing data online is the
norm.

Before any US-based SSP can be trusted to securing user data, it is
necessary for the third party doctrine to be abolished. Users should
have a reasonable expectation of privacy for any data they store
online, just as they would for any data stored on a hard drive at
their home. Fortunately there is movement toward the abolition of the
third party doctrine. On the judicial front, the Supreme Court has
suggested that it may be time to reevaluate the
principle~\cite{scotus-usvjones}. Congress also seems interested in
reevaluating the statue, making progress toward efforts such as
reforming the Electronic Communications Privacy Act (ECPA)~\cite{ecpa}
to include a warrant requirement for digitally stored
emails~\cite{eff-ecpareform}.

The abolition of the third party doctrine and the establishment of
traditional due process rights for digital data, even when stored with
third parties, is a necessary step toward developing a healthy and
trustworthy SSaaS ecosystem. Similar protections must be granted world
wide. Data stored via an SSP, regardless of jurisdiction, must be
respected just as data stored on a user themselves or in a safe in
their home.

\subsection{Privacy Breach Liability}
\label{chap:policy:trustworthiness:liability}

Beyond the need to treat SSP-stored data with the same privacy
protections as personally stored data lies the need to hold SSPs
liable for data breaches. If a SSP is breached and user secrets
exposed, the user should be able to hold the SSP liable and to seek
relief from the SSP for any damage that results. Such a liable would
follow the growing trend toward liability for digital data breaches
and poor user security practices (e.g.~\cite{ftc-asus}).

The nature of this liability could take several forms. The most
obvious form would be to impose civil liability commensurate with the
value of any exposed secrets on the party responsible for the secure
storage of such secrets. This opens up the thorny issue of how to
value the loss of a secret since some secrets may be worth very little
(e.g. an encryption key protecting a set of not particularly sensitive
family vacation photos) while others could be wroth quite a lot
(e.g. an encryption key protecting a trade secret or other sensitive
material). One way to overcome the valuation challenge would be to
have users declare the value of their secrets when they are stored,
similar to the manner in which one might declare the value of a parcel
when shipping it for the purpose of securing insurance. Given such a
declaration, the SSP could charge a user varying amounts: more
``valuable'' secrets should cost more to store, while less
``valuable'' secrets cost less. Damages in the event that a secret is
loss could then be calculated as a multiple of this value. In cases
where the loss results due to an unforeseeable event or otherwise
through no fault of the SSP, the user would be reimbursed the declared
value of the secret (or potentially some fraction of it). In the case
where a secret is lost due to SSP negligence, malpractice, or
malfeasance, the user would be reimbursed several multiples of the
secret value.

As mentioned in Chapter~\ref{chap:ssaas}, each SSP would likely be
required to secure insurance to cover the cost of such liability
payouts in the event that a breach occurs. These insurers, in turn,
would charge each SSP an insurance fee on the basis of how ``secure''
(or the inverse: how ``risky'') an SSP's storage practice are. Indeed,
it is even possible that the government itself might act as such an
insurer (or underwriter), as they currently do with banks via the
Federal Deposit Insurance Corporation (FDIC)~\cite{fdic}. The need to
acquire insurance to cover an SSP's liabilities will also impose a
natural limit on the size of any single SSP. Once an SSP's liability
grows beyond the size for which it find insurance, it would be forced
to stop accepting new customers (or more properly, new secrets). Such
a natural limit of the size and power of any single SSP is a healthy
property of a SSaaS ecosystem since it would encourage the competition
of a multitude of SSPs and would encourage users to shard their
secrets across multiple SSPs to minimize risk. Continuing the FDIC
analogy, this limit is similar to the \$250,000 per-account FDIC
liability limit which is designed to encourage user to spread their
money across multiple accounts held with multiple institutions,
limiting the trust placed in any single institutions.

The establishment of as civil liability for SSPs might also have
benefits in the ongoing ``forced decryption'' and key escrow
debates. The very nature of the SSaaS ecosystem likely makes it an
appealing model for those wishing to impose key escrow requirements
(see Key Escrow discussion in Chapter~\ref{chap:background}). Should
an SSaaS ecosystem become a standard means of storing secrets such as
user encryption keys, it is not inconceivable that the government
might wish to pass a requirement that a user escrow a copy of all of
their secrets with a government-controlled SSP for use in the event
that the government wishes to access a user's encrypted data. As
previously discussed, such a practice has a number of flaws and would
be bad public policy. Nonetheless, should the government wish to
pursue such a policy, having an established system of civil liability
for the loss user secret data would be useful. If such a system were
crafted to avoid exempting government-controlled SSPs from civil
liability requirements\footnote{Although the government does have a
  habit of exempting itself from such requirements, so establishing
  government liability for SSP-related data loss may be challenging.},
it could help minimize the number of keys the government chooses to
escrow. Indeed, such a system would impose a huge liability risk on
the government SSP as a concentrated holder of many secrets. This
liability risk attaches a real monetary value to the kinds of
existential escrow-related risks raised by security
professional~\cite{abelson2015}. Making such fears concrete encourages
the government to rethink the wisdom of such a system in the first
place. If the government can not afford to reimburse the range of
companies and individuals who would suffer damage in the event that a
government-controlled SSP were breached, they would be unable to
operate such an SSP in the first place.

Beyond civil liability, its also possible that it would be appropriate
to establish some degree of criminal liability for malpracticing or
negligent SSPs. Such liability could mirror existing criminal
malpractice and negligence laws such as those imposed on doctors,
contractors, and engineers. Such criminal liability would allow the
prosecution of particularly bad SSP operators and help to ensure that
the SSP market remains relatively safe for consumers wishing to
utilize the SSaaS ecosystem.

Regardless of mechanism, establishing a standard system and
expectation of secret breach liability will help to incentive security
best practices. It will provide users with a measure of relief in the
event that their secrets are leaked, increasing adoption of the SSaaS
ecosystem.

% Minimizing Forced Trust
\section{Openness and Transparency}

Thus far, this chapter has explored mechanisms to ensuring entities
within the SSaaS ecosystem remain secure and trustworthy. As
discussed throughout this document, however, it is also important to
provide end-users with tools for minimizing the trust they must place
in the SSaaS system or its actors. One mechanism for minimizing such
trust is to ensure that the code and protocols underlying the SSaaS
ecosystem remain open and transparent.

\subsection{Protocol Standardization and Transparency}

It is important that the specifications for any SSaaS protocols
(e.g. Custos, Tutamen, etc) remain open and encumber IP-related
restrictions. In such a process, several tenants must be
upheld:

\begin{packed_desc}
\item[Openness of Process:] The standardization process itself must be
  open to the widest possible range of individuals and encourage the
  participation of such individuals. Furthermore, the process must
  occur in public such that even those outside of the process may
  review and comment on the work undertaken within the process.
\item[Availability of Standards:] Both drafts and final versions of
  any standards must be freely and widely available under permissive
  licensing terms that allows the reproduction and distribution of
  said standards without requiring any licensing fees.
\item[Unencumbered by Intellectual Property Restrictions:] The
  technology discussed in a standard must be freely available or
  otherwise unencumbered by the need to secure individual intellectual
  property licenses or to pay licensing fees. It should be possible
  for outside parties to develop custom implementation of any standard
  and for end-users to utilize such implementations without either
  having to secure a license or pay a fee.
\end{packed_desc}

This set of requirements helps to ensure that standards can be widely
disseminated and adopted, and that a range of competing
implementations are such standards can be created and
distributed. Such process are already used by a variety of standards
setting bodies such as thus undertaken by the W3C~\cite{w3c} for the
development of web-related standards. Having a similarly open SSaaS
standardized process is important to a healthy SSaaS ecosystem in
order to encourage the adoption of a single SSaaS standard. Such a
standard, in turn, enables users to switch between multiple SSPs and
avoids the kinds of SSP lock-in issue proprietary SSaaS protocols
might cause. Having an open and reviewable protocol also reduces the
need for users to trust the protocol publisher directly by allowing
independent review.

\subsection{Free and Open Source Implementations}

Beyond the importance of developing SSaaS protocols in open and making
them freely available, it is also important that (at least some)
implementations of these protocols be open and freely available. Such
a requirement is secondary to having a free and open protocols, since
a free and open protocol is what allows the development of free and
open implementations in the first place. The use of such
implementations by SSPs will avoid forcing SSP users to trust that the
SSP has faithfully implemented the SSP protocol by facilitating the
review of such code by end end-users themselves (or third parties with
an interest in the end user's security).\footnote{Although this leaves
  open the problem of how to ensure that an SSP is running the open
  source software they claim to run, as opposed to running software
  that appears to be an open source variant to external observers, but
  that is actually a malicious or flawed clone. This problem might be
  dealt with via the insurance and liability issues previously
  discussed -- i.e. insures might require audits of an SSP's deployed
  code base and/or make their insurance programs contingent on the use
  of certain open source SSaaS protocol
  implementations. Alternatively, it is possible research from the
  ``reproducible builds'' space might assist in this
  task~\cite{DeCarnedeCarnavalet2014}. How to verify such reproducible
  builds for third party hosted software, however, remains an open
  problem.}

In order to be truly ``open'', an SSaaS implementation would need to
be both developed in the open and provide users with access to its
source code. Both forms of openness can be accomplished by hosting
code on a public source code reportedly such as GitHub~\cite{github}
and by conducting development discussions on an open mailing
list. Such openness ensures that users are free to review the
implementation of the SSaaS protocols themselves, reducing their need
to trust the software itself.

Beyond mere openness, it is also important that the software be Free
(as in freedom, not as in beer). In the open source community, such
freedom is generally premised on four core ``rights'': the right of
the user to run the software as they wish, the right of the user to
review and modify the software, the right of the user to redistribute
unmodified copies of the software, and the right of the user to
redistribute modified copies of the
software~\cite{fsf-freedomns}. Cloud-based software such as that
employed by SSPs also benefits from a fifth requirement: that the
service provider provide the use with a copy of the code they are
running~\cite{agpl}. Granting these rights to users helps to ensure
that they are free not only to independently review software, but also
to improve and redistribute a piece of software in the event that they
dislike what they find while reviewing it. These are important
properties since they further allows users to reduce their trust and
reliance on any single implementations of the SSaaS protocols.

While having a single standardized protocol is critical for the
development of a healthy SSaaS ecosystem, the opposite is true for
implementations of this standard. A healthy SSaaS ecosystem should
sport a variety of implementations (free/open or proprietary) of the
SSaaS protocol standard. Such a diversity of implementations allows
users to avoid having to trust any single implementation, and
decreases the insecurity that can result from a software
monoculture. Ensuring there at least a few core SSaaS implementation
strains remain free and open will help to encourage the development of
additional SSaaS strains, creating this desirable level of diversity.

\section{Use of Multiple SSPs}

While openness and transparency provide tools for reducing the trust
that users must place in SSaaS software and the ecosystem in general,
reducing trust in individual SSPs is best accomplished by sharding
user data across multiple SSPs. As discussed in
Chapter~\ref{chap:trust}, such sharding allows the user to distribute
their secrets in a manner such that no single SSP can derive any
meaningful information from the shard of the secret they hold. Since
sharding data across SSPs reduces the risk each individual SSP can
pose to a user, it is desirable to the healthy of the SSaaS ecosystem
to have policies which encourage such sharding. In order to accomplish
this goal, it is useful to consider the concept of secret sharding
from three main angles: limits regulators might impose on the
technology underlying and act of sharding, how SSP liability and
insurance relates sharded secrets, and the jurisdictional issues that
sharding a secret across multiple SSPs might raise.

\subsection{Limits on Data Sharding}

Similar to the previously discussed issues associated with limiting
the use of cryptography, any effort to limit or restrict a user's
ability to shard their secrets across multiple SSPs must be avoided in
order to ensure a healthy SSaaS ecosystem. There a number of reasons
regulators might attempt to limit such sharding. Foremost among these
reasons are that the algorithms underlying data sharding
(e.g. Shamir's~\cite{shamir1979}) might be interpreted as a form of
cryptography, with all the regulator baggage that may entail, and that
sharding data across multiple SSPs might violate various data
localization laws.

On the first point, whether or not secret sharding schemes would be
classified as ``cryptography'' for the purpose of cryptography-related
regulations is an open question. Existing US export control rules
governing cryptography do not contain a specific definition of what
consist a ``cryptographic'' system~\cite{bis-encrypted}. And a
traditional view of cryptography as a means for scrambling information
via a key doesn't necessarily apply to secret sharding systems since
such system include no key and aren't really ``scrambling''
information so much as selectively splitting it into multiple partial
copies with special properties.\footnote{In fact, secret sharing
  schemes are in a distinct class of algorithms than traditional
  cryptography schemes. Whereas traditional cryptography derives its
  strength from assumptions about the amount of computing power an
  adversary can bring to bear solving certain kinds of math problems
  (conditional security), secret sharing system derive their security
  from fundamental properties of information theory, and remain secure
  regardless of the amount of computational power an adversary posses
  (unconditional security).} Thus, it is possible that efforts to
limit the use of cryptography in general might not actually apply to
secret sharing schemes. Any effort to limit the use of secret sharding
schemes, just like any effort to limit the use of other forms of
``cryptography'', will prove detrimental to the SSaaS ecosystem. As
stated previously, ideally regulators will avoid trying to limit the
use of cryptography at all. Short of that, avoiding the classification
of secret strong schemes as cryptography for the purpose of
cryptography-related regulations is a desirable outcome science it
avoids sweeping such schemes into any limits imposed on more
traditional forms of cryptography.

On the second point, it is likely that laws aimed toward at mandating
the storage of data in a local jurisdiction will harm a user's ability
to effectively shard their secrets across multiple providers. Such
laws are generally presented as privacy-enhancing measures aimed at
ensuring third parties that store user data are subject to the laws of
the jurisdiction in which the user resides. In reality, however, such
laws are often used to maintaining less pure government-access
capabilities to user data (e.g.~\cite{dergacheva2015}). This is bad
for the SSaaS ecosystem for two reasons. First, any reduction in the
number of SSPs available for a user to chose from weakens the
competition-driven incentives of the SSaaS ecosystem. Limiting a user
to only storing data with local SSPs, or forcing SSPs to store data
for a user is a specific jurisdiction, limits the number of SSPs to
which a user has access.  Access to fewer SSPs means a smaller market
with less competition and thus a higher likelihood of poorly or
maliciously operated SSPs. Second, protecting user secrets from overly
intrusive government intersection is one of the roles of the SSaaS
model. Thus, any effort to limit a user's access to SSPs outside of
the jurisdiction in which theory reside will subvert one of the
potential SSaaS benefits. This idea is discussed further in
Section~\ref{chap:policy:trust:jurisdiction}.

In all cases, regulators should avoid polices what would decrease a
user's access secret sharding technology or that would limit the
number or diversity of secret storage providers to which a user has
access. Avoiding both regulations aimed at suppressing access to
secret sharing cryptography schemes and regulations encouraging the
``Balkanisation'' of the Internet into locally-controlled
sub-domains~\cite{lee2014} is necessary for encouraging a health SSaaS
ecosystem.

\subsection{Liability and Insurance}

Sharding secrets across multiple SSPs also has repercussions for the
liability and insurance ideas discussed in
Section~\ref{chap:policy:trustworthiness:liability}. The main question
secret sharding posses to SSP-related liability ideas in that of the
val us of a single secret shard. I.e. if an SSP is liable for leaks of
user data, how to we value the harm caused by leak of a single shard
of a secret?

Values for secret shards could range from being equal to the value of
the unsharded secret to being worth nothing at all. Making secret
shards worth as much as the secret itself makes since if you assume
that the exposure of any secret shard is an unacceptable risk since it
increases the risk of the exposure of the entire secret. On the other
extreme, values secret shards as worthless makes since if you assume
that each shard taken by itself provide no information about the
underlying secret, and is thus ``worthless'', even if leaked. It's
likely that the best solution lies somewhere in between. Valuing
secrets shards as equal to th value of an unsharded secret will likely
make the sharding of secrets cot prohibitive. Assuming that the
declared values of a secret is the driving mechanism an SSP Will use
to charge users for secret storage, then valuing secret shards the
same as secrets increases the users cost to store the secret on the
order of a factor of $k$, where $k$ is the number of SSPs a user
wishes to shared their secret across. This represents an undesirable
increase in cost. Similarly, valuing secret shards as worthless
ignores the realty that the leak of multiple shards can still reveal
the secret -- imposing the same harm on the user that the leak of an
unsharded secret would. But if shard are ''worthless'', than the user
will have no ability to seek restitution for the harms they suffer
from the SSPs responsible for leaking each shard. This is also
undesirable.

Intuitively, it would make the most sense for the act of secret
sharding to have no effective impact on either a user's cost to store
the secret more their ability to recoup losses if the secret shards
are leaked. To accomplish such as equivalency, it is necessary to
value each shard of a $k$ choose $n$ secret split as being worth $v/n$
where $v$ represents the cost of the unsharded secret and $n$
represents the number of shards necessary to reconstitute the
secret. In such a situation, the user's secret is effectively leaks
when $n$ shards are exposed. The exposure of $n$ shards allows the
user to seek damages for a value equal to $v/n$ from each of the $n$
SSPs, resulting in the same value $v$ they would be due in the
unsharded case. Similarly, the cots to the user to store each secret
shard should be equal to $c/n$ where $c$ is the cost to store the
unsharded secret. As in the liability valuation, such a split will
ensure the user pays the same for a sharded secret as an unsharded
secret. Since $k$ choose $n$ secret shadings schemes allow for the
user to specify a $k$ greater than $n$ for the purpose of adding
redundancy to the system, a user may still end up paying more to store
a sharded secret than an unsharded secret in any case where $k$ is
greater than $n$. In such situations, however, the user is gaining an
additional degree of redundancy that they did not have before,
justifying the additional cost. Thus, valuing secret shards at $v/n$
for the purpose of liability and charging for their storage at a rate
of $c/n$ will avoid financial or liability related disincentives to
sharding secrets across multiple providers.

The user can do even better than this in situations where the cost to
store a secret is exponentially, instead of linearly, related to the
declared value of the secret. E.g. it is not inconceivable that the
cost to store a secret valued \$1000 should be more than 10x the cost
to store a secret valued at \$10. Such a nonlinear cost vs value curve
can be justified by the fact that more expensive secrets are likely to
be more desirable targets for attackers, and thus command a higher
premium to store relative to their value. Assuming SSPs adopt such
non-linear cost regimens, sharding a secret become financially
beneficial to the user since it minimize the declared value of each
individual shard, reducing the total cost. Whereas a linear cost of
secret storage based on secret value results in the same total cost to
the user when sharding their secrets across multiple providers, a
non-linear secret storage cost can cause the user to realize a saving
by sharding their secrets, further encouraging the practice. It is
thus potentially desirable for SSPs to adopt non-linear secret storage
cost regimes where the cost to store a secret increases geometrically
or exponentially with its declared value. Such an ecosystem would
further encourage user to shard their secrets across multiple
providers, increasing user security while reading SSP trust and cost.

\subsection{Jurisdictional Issues}
\label{chap:policy:trust:jurisdiction}

%%  LocalWords:  SSaaS DES PGP's backdoored Slipjack Snowden NSA LEOs
%%  LocalWords:  Wassenaar nd th Tutamen CAs OpenSSL SSP SSPs Todo
%%  LocalWords:  ECPA SSP's Custos DIY GitHub Shamir's Balkanisation
