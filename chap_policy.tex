\chapter{Policy Implications}
\label{chap:policy}

The work presented in this document impinges a number of policy
questions in the security and privacy space. A number of these
implications have been alluded to in the previous chapters. This
chapter explores some of these policy questions in greater depth.

\section{Toward a Healthy SSaaS Ecosystem}

If the Secret Storage as a Service (SSaaS) ecosystem is to flourish,
the public must first be convinced that it is a viable solution for
increasing the security and privacy of the devices and services they
use (and that it does not have a detrimental impact on their ability
to utilize such devices and services). While policy initiatives can do
little to convince users that the SSaaS ecosystem does not impose an
undue burden on their desired use cases,\footnote{That's the job of
  previous chapters in this document, as well as of developers and
  marketers building SSaaS-backed solutions.} policy initiatives can
help to encourage the development of a healthy SSaaS
ecosystem. Establishing and maintaining a healthy SSaaS ecosystem
relies on three main components:

\begin{packed_desc}
\item[Availability of Cryptography:] Users must be able to access and
  use underlying SSaaS technologies before any SSaaS system can be
  deployed. An SSaaS ecosystem depends heavily on a range of
  cryptographic primitives, and these primitives are often the target
  of regulation. An inability to access or use such primitives will
  make an SSaaS ecosystem impossible to build
\item[Maximizing SSP Trustworthiness:] In order for the SSaaS
  ecosystem to flourish, user must believe that, in general, Secret
  Storage Providers (SSPs) are trustworthy actors. While a user can
  take steps to minimize how much they must trust each SSP (see next
  point), if SSPs are viewed as unreliable and untrustworthy in
  general, the SSaaS ecosystem will be tainted. In addition, user must
  have faith in the underlying SSaaS protocols and technologies.
\item[Minimizing Mandatory Trust:] While a degree of trust on both the
  SSaaS protocols as well as in each SSP is useful to help the SSaaS
  ecosystem thrive, it is also important for users to have tools for
  minimizing the degree to which they must trust any single part of or
  party in the SSaaS system. Such tools acts as the final security and
  privacy backstop for users, ensuring that their data might remain
  secure even if their trust in specific protocols or SSPs turns out
  to be misplaced.
\end{packed_desc}

There are a number of policy initiatives that will help to encourage
the development of these user views, and by proxy, the growth of a
health SSaaS ecosystem. Such an ecosystem severs the public interest
by increasing the security and privacy of users everywhere. These
initiatives are discussed throughout the reminder of this chapter.

\section{Availability of Cryptography}
\label{chap:policy:crypto}

Before an SSaaS system can thrive, it is critical that users have
access to the technologies underlying the SSaaS idea. This encompasses
a wide range of technologies from networking to cryptography. It is
cryptography, however, that tends to incite the most regulatory
actions, and thus it is the regulation of cryptography that will be
the focus of this section.

Many of the ideas proposed in this document rely on the use of a
variety of cryptographic primitives, including symmetric encryption,
message authentication codes (MAC), asymmetric encryption, asymmetric
signatures, etc. Such cryptographic primitives, however, have a
checkered legal and policy history, at least in the United
States\footnote{The bulk of this chapter will focus on U.S. policy and
  laws since that is the jurisdiction in which the author resides and
  is most intimately familiar. Similar ideas are applicable to other
  jurisdictions.}. This section provides an overview of
cryptography-related policy concerns.

\subsection{A Brief History of Cryptography Regulation}

Strong cryptography is a relatively recent development in the timeline
of human history. Modern digital cryptography didn't really come about
until the end World War II and the advent of information
theory~\cite{shannon1945}. Prior to that point in time, most
``cryptography'' systems were based on folk-theory, obscure languages,
mechanical devices, or other items not soundly rooted in mathematical
theory. Even though the groundwork was laid by the end of World War
II, modern practical digital cryptography systems didn't really become
available until the standardization of DES in the mid
1970s~\cite{fips46} and the invention of asymmetric cryptography
around the same time~\cite{diffie1976}. The combination of a
standardized symmetric key algorithm coupled with the ability to
negotiate and/or exchange symmetric keys over insecure channels using
asymmetric techniques made digital cryptography a practical tool for
securing communications. The security of such systems rests on the
provable difficulty of solving certain classes of mathematical
problems -- a far stronger security guarantee than that provided by
earlier systems.

From the late 1970s through the 1980s, digital cryptography remained
largely a tool for governments, militaries, and financial
corporations. As such, cryptography was often classified as a
``munition'' and placed under various arms and export control laws
regulating the manner in which encryption technology could be
distributed outside of the United States.\footnote{The U.S. did allow
  the export of certain types of purposely weak crypto, e.g. crypto
  with key lengths below 40 bits. This had the extremely unfortunate
  effect of encouraging the standardization and proliferation of
  various weak crypto algorithms in a wide range of consumer-facing
  software in order to allow the international distribution of such
  software. The proliferation of such ``export-grade'' cryptography
  still comes back to bite us today -- many applications still contain
  support for such algorithms, and that support makes those products
  vulnerable to ``downgrade'' attacks that trick the software into
  using this weak crypto. Recent vulnerabilities such as
  FREAK~\cite{beurdouche2015} and Logjam~\cite{adrian2015} are based
  on such attacks.} During this time, cryptographically secure systems
were not widely used by individuals not associated with the
aforementioned institutions.

The availability of strong cryptography to the general public begin to
increase in the early 1990s with the release of
PGP~\cite{zimmermann-pgp10}. PGP quickly became the first widely
distributed encryption software designed for use by ordinary
individuals.\footnote{Although, as mentioned in
  Chapter~\ref{chap:challenges}, the ability of ``ordinary''
  individuals to actually use PGP is highly
  questionable\cite{whitten1999}.}  PGP's Internet-based distribution
also quickly drew the attention of U.S. authorities who viewed such
publication as a violation of the export control laws banning the
export of cryptographic technologies with key lengths exceeding
40-bits. In defiance of such accusations, PGP's author published the
entire source code of PGP as a traditional bound book, the contents of
which could be scanned and compiled by anyone with a
copy~\cite{zimmermann-pgpsource}. This publication forced a discussion
of the First Amendment~\cite{us-constitution-amend1} issues associated
with the distribution of computer source code. Such discussions,
coupled with several court cases~\cite{ninthcir-bernstein,
  sixthcir-junger} in favor of the protection of the publication of
computer source code under the First Amendment, led to the U.S.
government rolling back (although not completely eliminating) most of
the export control rules surrounding cryptography by the early
2000s~\cite{kehl2015}.

During this same period, the government tried to standardize a
backdoored encryption system known as the Clipper
Chip~\cite{whitehouse-clipper}. The chip was designed to be embedded
in systems where it would provide ``strong'' encryption via the
Slipjack algorithm while maintaining the government's ability to
decrypt such encryption via the use of escrowed master keys. The
Clipper Chip quickly feel into disfavor, however, when researchers
demonstrated simple methods for bypassing its escrow
mechanisms~\cite{blaze1994}. The publication of such mechanisms was
soon followed by discoveries of weaknesses in the Slipjack
algorithm~\cite{biham1998}. Due to these flaws, and a general distrust
of any system requiring the escrowing of encryption keys with the
government, the Clipper chip was never widely adopted and the
government largely gave up on pushing backdoored encryption systems by
the start of the 2000s.

After the failure of both the government's export control based
cryptography regulations and their push for key escrow-based
encryption standards, efforts to control and regulate cryptography
were largely quite through the 2000s and early 2010s. Beyond the
government's previous failures, this regulatory silence was also due
to the fact that the use of encryption such a PGP by end users never
really expanded to the levels the government seemed most concerned
about. The usability challenges of encryption technology itself,
coupled with the largely apathetic privacy views by many members of
the general public, proved far more effective at limit the widespread
use of encryption than any government regulation.

Recently, however, this has begun to change. The Edward Snowden
revelations of spying abuses by the U.S. National Security Agency
(NSA) in the early 2010s led to a rise in privacy awareness among the
general populace~\cite{pew-privsec14}. This, in turn, led a push to
increase the use of strong cryptography across a wide range of
consumer-oriented products, from websites~\cite{mozilla-deprecatehttp}
to email~\cite{gmail-blog-encryption} to
smartphones~\cite{ars-ios-encrypt, ars-android-encrypt}. The
associated increase in end user use of cryptography has led to renewed
calls by law enforcement agencies around the world (including the U.S.
FBI~\cite{comey-testimony-encryption}) to mandate the use of breakable
encryption in any scenario were a government feels they are justified
in accessing the underlying data (e.g. when issued a probably cause
warrant). Such calls have culminated in the ongoing court battle
between Apple and the FBI over whether or not the FBI can use the All
Writs Act~\cite{usc-allwrits} to compel Apple to modify the iPhone
operating system to expedite the rate at which the FBI can guess the
pass-code required to decrypt a phone~\cite{ars-cookvfbi}. Congress
has also responded to such calls by suggesting legislation ranging
from the formation of encryption ``study'' committees~\cite{hr4651} to
outright bans on the use of strong encryption~\cite{bennett-burrbill}
to bans on such bans of encryption~\cite{hr4528}. How these court
cases and legislative actions will play out remains to be seen.

\subsection{A Defense of Cryptography}

From the export-control bans of the 1990s~\cite{kehl2015} to the Apple
v FBI crypto debate today, whether or not strong cryptography should
be available to the general public is a hotly contested topic. Should
the government ever succeed in banning or strongly curtailing the
development, distribution, or use of strong cryptography, it will
become substantially more difficult (if not impossible) to securely
implement SSaaS-related ideas, and by proxy, the privacy and security
enhancements SSaaS systems might provide. Indeed, any effort that
curtails the use of or access to strong cryptography is likely to
damage the security and privacy of users across a wide range of
digital systems.

From a policy perspective, there are a variety arguments against
efforts to ban or regulate various form of cryptography. They tend to
fall along a three standard tracks.

\begin{packed_desc}
\item[More Harm Than Good] \hfill \\ As discussed in this document and
  elsewhere~\cite{abelson2015}, cryptography is a critical tool for
  protecting the security and privacy of modern computing
  systems. Thus, any effort aimed at weakening cryptography for the
  purpose of minimizing its effectiveness to ``bad'' actors is just as
  likely to minimizing its effectiveness to ``good'' actors. And many
  more ``good'' actors rely on cryptography than do ``bad''
  actors. Efforts to curtail the use of strong cryptography would
  severely damage the security of financial, communication, and data
  storage systems. Any backdoor available to the U.S. government is an
  additional point of failure that an attacker may use to compromise
  and access a system. Any policy of providing the U.S. government
  with special access to cryptographically protected data will likely
  be mirrored by other countries -- many of which have far weaker
  legal protections against the abuse of such powers than does the
  U.S.\footnote{E.g. if the U.S. government can demand special access,
    so can China or North Korea -- weakening the security of
    U.S. citizens abroad or anyone wishing to secure their selves and
    systems from potentially repressive regimes.} Bans of cryptography
  thus trade the security of a large number of normal individuals for
  the insecurity of a small number of potentially bad actors -- and
  that's not a trade that's worth making.
\item[Ineffectiveness] \hfill \\ Beyond the fact that bans on
  cryptography are likely to cause more harm than good, such bans are
  widely believed to be ineffective. Even if the U.S. manages to ban
  the use of cryptography, cryptographic concepts are no longer a
  niche topic. There are hundreds cryptographic products available
  worldwide, more than half of which are developed and distributed
  outside of the Untied States~\cite{schneier2016}. The Internet and
  modern commerce systems make such products widely available to
  almost anyone. Thus, even if the U.S. manages to ban cryptography,
  those wishing to use it will just obtain cryptography-related
  products elsewhere. Modern cryptographic concepts are also widely
  understood and taught~\cite{schneier2010crypto}. There are thus a
  wide variety of individuals capable of creating cryptographically
  secure systems from scratch.\footnote{Although it is not advisable
    for most individuals to attempt to ``roll their own'' cryptography
    if they can avoid needing to do so. Such DIY low-level
    cryptographic systems have a historically high likelihood of
    mistakes due to the lack of formal vetting and widespread review.}
  Thus, it would not be difficult for organizations or individuals
  wishing to employ banned cryptography concepts to find someone to
  build such systems for them.
\item[Better Options] \hfill \\ Law enforcement organizations (LEOs)
  often cite the ``going dark'' problem as the reason why
  cryptographic restrictions are necessary. ``Going dark'' refers to
  the idea that as the use of cryptography grows the ability of law
  enforcement to monitor criminal activity for the purpose of
  preventing or prosecuting crimes
  diminishes~\cite{anderson2013}. What this viewpoint fails to
  acknowledge is the fact that this is the golden age of surveillance:
  law enforcement agencies have access to more data than they ever
  have had before~\cite{swire2011}. Thus, even with the widespread use
  of encryption, LEOs have more access to data about a potential
  criminal than at any prior point in human history. This fact
  presents a multitude of ways to combat crime without weakening
  cryptography. Such mechanisms may require more time and effort than
  the kind of large scale automated digital surveillance the use of
  cryptography subverts, but such time and effort is a necessary price
  to pay to ensure a proper balance between privacy and security.
  Instead of weakening encryption, law enforcement can focus their
  efforts on targeted investigations of specific individuals,
  leveraging the large digital footprints even cryptography-employing
  users leave behind. And even when using cryptography, individuals
  are still prone to a wide range of targeted attacks, from efforts to
  compromise individual systems, and subvert any cryptography they may
  employ, to social engineering attacks that tend to be very effective
  against even cryptography employing adversaries. There are numerous
  tools available to law enforcement to pursue criminals. Weakening
  encryption need not be one of them.
\end{packed_desc}

Beyond policy arguments against regulating or banning cryptography,
there are also a number of legal arguments preventing such efforts. In
particular, any effort to ban cryptography would raise serious
constitutional questions.

\subsubsection{1\textsuperscript{st} Amendment Issues}

As previously mentioned, bans on the publication of source code,
cryptography-related or otherwise, raise numerous
1\textsuperscript{st} Amendment questions. The free speech guarantees
provided by the 1\textsuperscript{st} Amendment have generally been
interpreted to apply to source code~\cite{ninthcir-bernstein,
  sixthcir-junger}.\footnote{Beyond cryptography, this ``code as
  speech'' interpretation is fairly critical to the use of
  U.S. Copyright Law for the protection of code-related intellectual
  property.} As such, any ban on cryptography source code is likely an
unconstitutional prior restraint on the author's freedom of
speech. Traditionally, courts have taken a dim few of laws imposing
such prior restraint~\cite{scotus-nearvminnesota}, even in cases where
national security may be impinged~\cite{scotus-nytvus}.

Additionally, efforts to mandate the inclusion of a backdoor in
certain software are likely a form of unconstitutional compelled
speech. Just as blocking the publication of source code interferes
with the author's free speech rights, mandating that an author
publishes or includes certain components in their code is also a
violation of their free speech
rights~\cite{scotus-wooleyvmaynard}. While there have been exceptions
to such interpretations, it is unlikely the broadness of any general
limits of cryptography, coupled with the traditional ineffectiveness
of such limits, would meet the kind of ``narrowly tailored limits in
effective support of a legitimate government interest'' requirement
such a law would have to overcome to meet constitutional muster.

Still open to debate, however, is whether or not 1\textsuperscript{st}
Amendment protections are limited merely to the source code
implementing encryption, or whether they apply to executable binaries
built from such code as well. If such protections are limited to raw
source code, it could effectively prevent the widespread use of
cryptography to those capable of acquiring and building source
code. Such an outcome would have a detrimental impact on the general
populace who often lack the ability to perform such actions.

\subsubsection{2\textsuperscript{nd} Amendment Issues}

One of the more creative constitutional defenses of
cryptography~\cite{xkcd-504} comes via the 2\textsuperscript{nd}
Amendment's grant to citizens of the right to ``keep and bear
arms''~\cite{us-constitution-amend2}. The idea is that since the
government already has a history of considering cryptography to be a
weapon, then the 2\textsuperscript{nd} Amendment should be used to
protect the rights of citizens to posses and use cryptography. This
theory has never been tested in a court, but it is not entirely
unreasonable to believe that cryptography is a necessary component of
maintaining a ``well regulated militia'' today, and thus should be
eligible for 2\textsuperscript{nd}
Amendment~\cite{scotus-usvmiller-guns} protections. It is also not
unreasonable to assume that if the Constitution were being written
today, cryptography would seem a much more natural fit for protection
via something like the second amendment then firearms.

Whether or not this argument will pass judicial scrutiny remains to be
seen. And even if it does, the 2\textsuperscript{nd} Amendment is
likely to guarantee a weaker set of rights than the
1\textsuperscript{st} Amendment: the 2\textsuperscript{nd} Amendment
only protects the right to ``keep and bear'' cryptography, where as
the 1\textsuperscript{st} Amendment also protects the right to
distribute cryptography. It may be possible, however, to take a split
approach to protecting cryptography where cryptographic source code
and research are protected via the 1\textsuperscript{st} Amendment
while binary programs or hardware employing cryptography are protected
via the 2\textsuperscript{nd} Amendment -- similar in mechanism to how
the 1\textsuperscript{st} Amendment would protect the publication of a
design of a firearm while the 2\textsuperscript{nd} Amendment would
protect the possession of the firearm itself.

\subsubsection{4\textsuperscript{th} Amendment Issues}

Many law enforcement agencies cite the importance of being able to
access information for which they have secured a probable cause
warrant as their reason for wishing to restrict the use of
cryptography. Such opponents fear the ability of cryptography to
render such lawfully obtained warrants useless. Whether or not the
4\textsuperscript{th} Amendment~\cite{us-constitution-amend4} can be
used to compel a service provider to decrypt data in order to comply
with a valid warrant remains to be seen. Traditional interpretations
hold that if a third party holds the key necessary to decrypt a user's
data (or the data itself), then they can be compelled to provide it to
the government -- often without requiring a probable cause warrant at
all~\cite{thompson-thirdparty}. But what about cases where the third
party lacks any ability to decrypt the data? Can they still be forced
to aid the government in serving a valid warrant?  The answer to this
question probably hinges on the ``reasonableness'' of such a
request. Should litigation such as the Apple v. FBI ever go to trail,
it may help shed some light on this question.

Cryptography, however, need not be viewed as antithetic to the
4\textsuperscript{th} Amendment. In fact it would seem that
cryptography provides the ultimate tool for protecting an individual's
4\textsuperscript{th} Amendment rights since such technology helps
ensure that individuals remain ``secure in their persons, houses,
papers, and effects''. But how best to reconcile this protection with
a valid ``probable cause'' warrant remains a matter of debate. And
ultimately, as in cases where only the end user posses the information
necessary to decrypt communication, the answer to this question may
have more bearing on the 5\textsuperscript{th} Amendment than the
4\textsuperscript{th}. But it seems unlikely that the need to serve a
4\textsuperscript{th} Amendment warrant would ever justify the
violations of the 1\textsuperscript{st} and 2\textsuperscript{nd}
Amendments that any general ban on the use of cryptography would
entail.

\subsubsection{5\textsuperscript{th} Amendment Issues}

Assuming the 1\textsuperscript{st} and 2\textsuperscript{nd}
Amendments can protects an individual's right to distribute, posses,
and use cryptography, then it falls on the 5\textsuperscript{th}
Amendment to protect an individual's right not to be forced to
disclose the necessary password or key required to decrypt a file or
sign a document. The right against self incrimination has long been
held to protect individuals against compelled testimony -- which
should include being forced to provide a memorized password or
key.\footnote{If the user has written down their password or locked
  their phone with a fingerprint, 5\textsuperscript{th} Amendment
  protections no longer apply since the evidence is then physical, not
  testimonial.}  Courts, however, are currently split on whether or
not the 5\textsuperscript{th} Amendment protects individuals from
being forced to turn over cryptographic keys or otherwise provide
decryption assistance~\cite{usvboucher, commonwealthvgelfgatt,
  usvdoe}.

Further complicating matters: certain types of cryptography can be
designed to decrypt a ciphertext to different plaintexts depending on
the key provided. Thus, it is possible that an individual could
provide one password to decrypt a file to a picture of a cat and
another password to decrypt a file to a plan for robbing a
bank. Additional passwords could provide additional alternative
contents. In such an arrangement, there is no reliable way to know how
many different plaintexts are hidden within a given ciphertext. Thus,
even if the government does succeed in compelling an individual to
``decrypt'' their documents, it is entirely possible they would still
wind up with a set of files other than the ones they were seeking.  It
seems unlikely any court would allow the government to continue
compelling a suspect to provide additional keys until they happened to
get a decrypted set of files that favored their case, especially since
it is possible no such files exist at all.

The 5\textsuperscript{th} Amendment has historically protected the
contents of an individuals mind. Cryptography (via encryption) allows
an individual to expand the contents of their mind by leveraging a
small piece of memorized information (i.e. a password or key) to
protect much larger pieces of offloaded information (e.g. a hard drive
full of files). Whether or not such an expansion is permissible within
the U.S. legal system remains to be decided.\footnote{Deciding whether
  or not such expansions are acceptable using cryptography will likely
  have a direct implication on the potential future world where human
  brains are technologically upgradeable. Indeed, there is likely
  little functional difference between using a cryptographically
  protected hard drive coupled with a memorized passcode to expand
  one's ``memory'' and installing a brain-linked computer chip that
  expands one's memory. In a world were you have the technology to
  upgrade the the amount of data your mind can store (and potentially
  where the government has the technology to ``mind-read'' by
  downloading such data), does the 5\textsuperscript{th} Amendment
  still protect the contents of one's mind? While such questions
  remain largely academic today, it may not be long before courts are
  being asked to rule on them.}

There are clear policy reasons to avoid curtailing or discouraging the
use of cryptography, as well as clear legal hurtles to doing so. It is
critical that cryptography remains widely and freely available to all,
since any reduction in access to cryptography would have severe
consequence on a range of security enhancing and privacy preserving
technologies. This includes on the SSaaS ideas proposed in this
document.

\section{Maximizing SSP Trustworthiness}

In order to instill faith in the SSaaS ecosystem, it is important that
users view Secret Storage providers (SSPs) as trustworthy entities
worthy of protecting user data. At the same time, SSPs are likely to
be appealing targets for both legal and extralegal introspection.
Whether such services are storing encryption keys for user data or raw
user secrets, they will be subjected to a range of government
information requests, criminal hacking attempts, and general
malfeasance. It is important that SSPs are able to stand up to this
barrage of data access attempts and keep the data they store
secure. Failure to due so risks soiling the SSaaS ``brand''. This fact
suggests a number of policy-related questions and potential solutions.

\subsection{Third Party Privacy}

SSP servers are most likely to be operated by third parties. As such,
they are likely to be subject to the third party
doctrine~\cite{thompson-thirdparty}. This doctrine holds that
individuals who voluntarily store their data with third parties have
no ``reasonable expectation of privacy''~\cite{scotus-katzvus} for
such data. While such a viewpoint may have made since in the mid 20th
century when it was established via a series of Supreme Court
rulings~\cite{scotus-usvmiller-privacy, scotus-smithvmaryland}, it
does not translate well to a world where storing data with third
parties online is the norm.

Before any U.S.-based SSP can be trusted to secure user data, it is
necessary for the third party doctrine to be abolished. Users should
have a reasonable expectation of privacy for any data they store
online, just as they would for any data stored on a hard drive at
their home. Fortunately there is movement toward the abolition of the
third party doctrine. On the judicial front, the Supreme Court has
suggested that it may be time to reevaluate the
principle~\cite{scotus-usvjones}. Congress also seems interested in
reevaluating the statue, making progress toward efforts such as
reforming the Electronic Communications Privacy Act (ECPA)~\cite{ecpa}
to include a warrant requirement for digitally stored
emails~\cite{eff-ecpareform}.

The abolition of the third party doctrine and the establishment of
traditional due process rights for digital data, even when stored with
third parties, is a necessary step toward developing a healthy and
trustworthy SSaaS ecosystem. Similar protections must be granted world
wide. Data stored via an SSP, regardless of jurisdiction, must be
afforded the same degree to respect and legal protection as data
stored in a safe at a user's home.

\subsection{Privacy Breach Liability}
\label{chap:policy:trustworthiness:liability}

Beyond the need to treat SSP-stored data with the same privacy
protections as personally stored data lies the need to hold SSPs
liable for data breaches. If a SSP is breached and user secrets
exposed, the user should be able to hold the SSP liable and to seek
relief from the SSP for any damage that results. Such liability would
follow the growing trend toward holding companies liable for digital
data breaches resulting from poor security practices
(e.g.~\cite{ftc-asus}).

The nature of this liability could take several forms. The most
obvious form would be to impose civil liability commensurate with the
value of any exposed secrets on the party responsible for the secure
storage of such secrets. This opens up the thorny issue of how to
value the loss of a secret. Clearly, some secrets may be worth very
little (e.g. an encryption key protecting a set of not particularly
sensitive family vacation photos) while others could be wroth quite a
lot (e.g. an encryption key protecting a trade secret or other
sensitive material). One way to overcome the valuation challenge would
be to have users declare the value of their secrets when they are
stored, similar to the manner in which one might declare the value of
a parcel when shipping it for the purpose of securing insurance. Given
such a declaration, the SSP could charge a user varying amounts: more
``valuable'' secrets should cost more to store, while less
``valuable'' secrets cost less. Damages in the event that a secret is
lost could then be calculated as a multiple of this value. In cases
where the loss results due to an unforeseeable event or otherwise
through no fault of the SSP, the user would be reimbursed the declared
value of the secret (or potentially some fraction of it). In the case
where a secret is lost due to SSP negligence, malpractice, or
malfeasance, the user would be reimbursed several multiples of the
secret value.

As mentioned in Chapter~\ref{chap:ssaas}, each SSP would likely be
required to secure insurance to cover the cost of such liability
payouts in the event that a breach occurs. These insurers, in turn,
would charge each SSP an insurance fee on the basis of how ``secure''
(or the inverse: how ``risky'') an SSP's storage practice are. Indeed,
it is even possible that the government itself might act as such an
insurer (or underwriter), as they currently do with banks via the
Federal Deposit Insurance Corporation (FDIC)~\cite{fdic}. The need to
acquire insurance to cover an SSP's liabilities will also impose a
natural limit on the size of any single SSP. Once an SSP's liability
grows beyond the size for which it find insurance, it would be forced
to stop accepting new customers (or more properly, new secrets). Such
a natural limit on the size and power of any single SSP is a healthy
property of a SSaaS ecosystem since it would encourage the competition
of a multitude of SSPs and would encourage users to shard their
secrets across multiple SSPs to minimize risk. Continuing the FDIC
analogy, such a limit is similar to the \$250,000 per-account FDIC
liability limit which is designed to encourage user to spread their
money across multiple accounts held with multiple institutions,
limiting the trust placed in any single institutions.

The establishment of as civil liability for SSPs might also have
benefits in the ongoing ``forced decryption'' and key escrow
debates. The very nature of the SSaaS ecosystem likely makes it an
appealing model for those wishing to impose key escrow requirements
(see Key Escrow discussion in Chapter~\ref{chap:background}). Should
an SSaaS ecosystem become a standard means of storing secrets such as
user encryption keys, it is not inconceivable that the government
might wish to pass a requirement that a user escrow a copy of all of
their secrets with a government-controlled SSP for use in the event
that the government wishes to access a user's encrypted data. As
previously discussed, such a practice has a number of flaws and would
be bad public policy. Nonetheless, should the government wish to
pursue such a policy, having an established system of civil liability
for the loss of user secret data would be useful. If such a system
were crafted to avoid exempting government-controlled SSPs from civil
liability requirements\footnote{Although the government does have a
  habit of exempting itself from such requirements~\cite{sisk2011}, so
  establishing government liability for SSP-related data loss may be
  challenging.}, it could help minimize the number of keys the
government chooses to escrow. Indeed, such a system would impose a
huge liability risk on the government SSP as a concentrated holder of
many secrets. This liability risk attaches a real monetary value to
the kinds of existential escrow-related risks raised by security
professional~\cite{abelson2015}. Making such fears concrete encourages
the government to rethink the wisdom of such a system in the first
place. If the government can not afford to reimburse the range of
companies and individuals who would suffer damage in the event that a
government-controlled SSP were breached, they would be unable to
operate such an SSP in the first place.

Beyond civil liability, its also possible that it would be appropriate
to establish some degree of criminal liability for malpracticing or
negligent SSPs. Such liability could mirror existing criminal
malpractice and negligence laws such as those imposed on doctors,
contractors, and engineers. Such criminal liability would allow the
prosecution of particularly bad SSP operators and help to ensure that
the SSP market remains relatively safe for consumers wishing to
utilize the SSaaS ecosystem.

Regardless of mechanism, establishing a standard system and
expectation of secret breach liability will help to incentive security
best practices. Fostering policies encouraging the development of a
robust cyber liability security market will further incentivize best
practices~\cite{starks2016}. Such practices provide users with a
measure of relief in the event that their secrets are leaked,
increasing adoption of the SSaaS ecosystem.

\section{Minimizing Mandatory Trust}

Thus far, this chapter has explored mechanisms to ensuring entities
within the SSaaS ecosystem remain secure and trustworthy. As discussed
throughout this document, however, it is also important to provide
end users with tools for minimizing the trust they must place in the
SSaaS system or its actors. 

\subsection{Protocol Standardization and Transparency}

One mechanism for minimizing trust is to ensure that the code and
protocols underlying the SSaaS ecosystem (e.g. Custos, Tutamen, etc)
remain transparent, open, and unencumbered by IP-related
restrictions. These properties will ensure that SSaaS protocols and
software can spread and improve over time. In order to achieve these
goals, the SSaaS ecosystem must strive for several goals:

\begin{packed_desc}
\item[Openness of Process:] The standardization process itself must be
  open to the widest possible range of individuals and encourage the
  participation of such individuals. Furthermore, the process must
  occur in public such that even those outside of the process may
  review and comment on the work undertaken within the process.
\item[Availability of Standards:] Both drafts and final versions of
  any standards must be freely and widely available under permissive
  licensing terms that allows the reproduction and distribution of
  said standards without requiring any licensing fees.
\item[Unencumbered by Intellectual Property Restrictions:] The
  technology discussed in a standard must be freely available or
  otherwise unencumbered by the need to secure individual intellectual
  property licenses or to pay licensing fees. It should be possible
  for outside parties to develop custom implementation of any standard
  and for end users to utilize such implementations without either
  having to secure a license or pay a fee.
\end{packed_desc}

This set of requirements helps to ensure that standards can be widely
disseminated and adopted, and that a range of competing
implementations for such standards can be created and
distributed. Such process are already used by a variety of standards
setting bodies such as thus undertaken by the W3C~\cite{w3c} for the
development of web-related standards. Having a similarly open SSaaS
standardized process is important to a healthy SSaaS ecosystem in
order to encourage the adoption of a single SSaaS standard. Such a
standard, in turn, enables users to switch between multiple SSPs and
avoids the kinds of SSP lock-in issue proprietary SSaaS protocols
might cause. Having an open and reviewable protocol also reduces the
need for users to trust the protocol publisher directly by allowing
independent review.

\subsection{Free and Open Source Implementations}

Beyond the importance of developing SSaaS protocols in open and making
them freely available, it is also important that (at least some)
implementations of these protocols be open and freely available. Such
a requirement is secondary to having a free and open protocol, since a
free and open protocol is what allows the development of free and open
implementations in the first place. The use of such implementations by
SSPs will avoid forcing SSP users to trust that the SSP has faithfully
implemented the SSP protocol by facilitating the review of such code
by end users themselves (or third parties with an interest in the end
user's security).\footnote{Although this leaves open the problem of
  how to ensure that an SSP is running the open source software they
  claim to run, as opposed to running software that appears to be an
  open source variant to external observers, but that is actually a
  malicious or flawed clone. This problem might be dealt with via the
  insurance and liability issues previously discussed -- i.e. insures
  might require audits of an SSP's deployed code base and/or make
  their insurance programs contingent on the use of certain open
  source SSaaS protocol implementations. Alternatively, it is possible
  research from the ``reproducible builds'' space might assist in this
  task~\cite{DeCarnedeCarnavalet2014}. How to verify such reproducible
  builds for third party hosted software, however, remains an open
  problem.}

In order to be truly ``open'', an SSaaS implementation would need to
be both developed in the open and provide users with access to its
source code. Such openness can be accomplished by hosting code on a
public source code reportedly such as GitHub~\cite{github} and by
conducting development discussions on an open mailing list. These
forms of openness ensure that users are free to review the
implementation of the SSaaS protocols themselves, reducing their need
to trust the software itself.

Beyond mere openness, it is also important that the software be
Free.\footnote{``Free'' as in freedom, not ``free'' as in beer.}  In
the open source community, such freedom is generally premised on four
core ``rights'': the right of the user to run the software as they
wish, the right of the user to review and modify the software, the
right of the user to redistribute unmodified copies of the software,
and the right of the user to redistribute modified copies of the
software~\cite{fsf-freedoms}. Cloud-based software such as that
employed by SSPs also benefits from a fifth right: the right of users
of the software to obtain a copy of the source code powering the
software itself~\cite{agpl}. Granting these rights to users helps to
ensure that they are free not only to independently review software,
but also to improve and redistribute a piece of software in the event
that they dislike what they find while reviewing it. These are
important properties since they further allows users to reduce their
trust and reliance on any single implementation of the SSaaS
protocols.

While having a single standardized protocol is critical for the
development of a healthy SSaaS ecosystem, the opposite is true for
implementations of this standard. A healthy SSaaS ecosystem should
support a variety of implementations (free/open or proprietary) of the
SSaaS protocol standard. Such a diversity of implementations allows
users to avoid having to trust any single implementation, and
decreases the insecurity that can result from a software
monoculture. Ensuring there at least a few core SSaaS implementation
strains remain free and open will help to encourage the development of
additional SSaaS strains, creating this desirable level of diversity.

\subsection{Use of Multiple SSPs}

While openness and transparency provide tools for reducing the trust
that users must place in SSaaS software and the ecosystem in general,
reducing trust in individual SSPs is best accomplished by sharding
user data across multiple SSPs. As discussed in
Chapter~\ref{chap:trust}, such sharding allows the user to distribute
their secrets in a manner such that no single SSP can derive any
meaningful information from the shard of the secret they hold. Since
sharding data across SSPs reduces the risk each individual SSP can
pose to a user, it is desirable to the healthy of the SSaaS ecosystem
to have policies which encourage such sharding. To accomplish this
goal, it is useful to consider the concept of secret sharding from
three main angles: limits regulators might impose on the technology
underlying and act of sharding, how SSP liability and insurance
relates to sharded secrets, and the jurisdictional issues that
sharding a secret across multiple SSPs might raise.

\subsubsection{Limits on Data Sharding}

Similar to the previously discussed issues associated with limiting
the use of cryptography, any effort to limit or restrict a user's
ability to shard their secrets across multiple SSPs must be
avoided. There a number of reasons regulators might attempt to limit
such sharding. Foremost among these reasons are that the algorithms
underlying data sharding (e.g. Shamir's~\cite{shamir1979}) might be
interpreted as a form of cryptography, with all the regulatory baggage
that may entail, and that sharding data across multiple SSPs might
violate various data localization laws.

On the first point, whether or not secret sharding schemes would be
classified as ``cryptography'' for the purpose of cryptography-related
regulations is an open question. Existing U.S. export control rules
governing cryptography do not contain a specific definition of what
constitutes a ``cryptographic'' system~\cite{bis-encrypted}. A
traditional view of cryptography as a means for scrambling information
via a key doesn't necessarily apply to secret sharding systems since
such systems include no key and aren't really ``scrambling''
information so much as selectively splitting it into multiple partial
copies with special properties.\footnote{As discussed in
  Chapter~\ref{chap:background} secret sharing schemes are in a
  distinct class of algorithms than traditional cryptography
  schemes. Whereas traditional cryptography derives its strength from
  assumptions about the amount of computing power an adversary can
  bring to bear solving certain kinds of math problems (conditional
  security), secret sharing system derive their security from
  fundamental properties of information theory, and remain secure
  regardless of the amount of computational power an adversary posses
  (unconditional security).} Thus, it is possible that efforts to
limit the use of cryptography in general might not actually apply to
secret sharing schemes. Nonetheless, any effort to limit the use of
secret sharding schemes, just like any effort to limit the use of
other forms of ``cryptography'', will prove detrimental to the SSaaS
ecosystem. As stated previously, ideally regulators will avoid trying
to limit the use of cryptography at all. Short of that, avoiding the
classification of secret strong schemes as cryptography for the
purpose of cryptography-related regulations is a desirable outcome
since it avoids sweeping such schemes into any limits imposed on more
traditional forms of cryptography.

On the second point, it is likely that laws aimed at mandating the
storage of data in a local jurisdiction (e.g.~\cite{dergacheva2015})
will harm a user's ability to effectively shard their secrets across
multiple providers. Such laws are generally presented as
privacy-enhancing measures aimed at ensuring third parties that store
user data are subject to the laws of the jurisdiction in which the
user resides. In reality, however, data localization laws are often
used for less pure purposes such as maintaining government-access
capabilities to user data. This is bad for the SSaaS ecosystem for two
reasons. First, any reduction in the number of SSPs available for a
user to chose from weakens the competition-driven incentives of the
SSaaS ecosystem. Limiting a user to only storing data with local SSPs,
or forcing SSPs to store data for a user in a specific jurisdiction,
limits the number of SSPs to which a user has access.  Access to fewer
SSPs means a smaller market with less competition and thus a higher
likelihood of poorly or maliciously operated SSPs. Second, protecting
user secrets from overly intrusive governments is one of the roles of
the SSaaS model. Thus, any effort to limit a user's access to SSPs
outside of the jurisdiction in which they reside will subvert one of
the potential SSaaS benefits. This idea is discussed further in
Section~\ref{chap:policy:trust:jurisdiction}.

In all cases, regulators should avoid polices what would decrease a
user's access to secret sharding technology or that would limit the
number or diversity of secret storage providers to which a user has
access. Avoiding both regulations aimed at suppressing access to
secret sharing schemes and regulations encouraging the
``Balkanisation'' of the Internet into locally-controlled
sub-domains~\cite{lee2014} is necessary for encouraging a healthy
SSaaS ecosystem.

\subsubsection{Liability and Insurance}

Sharding secrets across multiple SSPs also has repercussions for the
liability and insurance ideas discussed in
Section~\ref{chap:policy:trustworthiness:liability}. The main question
secret sharding poses to SSP-related liability ideas is that of the
value of a single secret shard. I.e. if an SSP is liable for leaks of
user secrets, how do we value the harm caused by leaks of a single
shard of a secret?

Values for secret shards could range from being equal to the value of
the unsharded secret to being worth nothing at all. Making secret
shards worth as much as the secret itself makes since if you assume
that the exposure of any secret shard is an unacceptable risk since it
increases the risk of the exposure of the entire secret. On the other
extreme, valuing secret shards as worthless makes since if you assume
that each shard taken by itself provides no information about the
underlying secret, and is thus ``worthless'', even if leaked. It is
likely that the best solution lies somewhere in between. Valuing
secrets shards as equal to the value of an unsharded secret will
likely make the sharding of secrets cost prohibitive. Assuming that
the declared value of a secret is the driving mechanism an SSP will
use to charge users for secret storage, then valuing secret shards the
same as secrets increases the user's cost to store the secret on the
order of a factor of $n$, where $n$ is the number of SSPs a user
wishes to shared their secret across. This represents an undesirable
increase in cost. Similarly, valuing secret shards as worthless
ignores the realty that the leak of multiple shards can still reveal
the secret -- imposing the same harm on the user that the leak of an
unsharded secret would. But if shards are ''worthless'', than the user
will have no ability to seek restitution for the harms they suffer
from the SSPs responsible for leaking each shard. This is also
undesirable.

Intuitively, it would make the most sense for the act of secret
sharding to have no effective impact on either a user's cost to store
the secret nor their ability to recoup losses if the secret shards are
leaked. To accomplish such an equivalency, it is necessary to value
each shard of a $n$ choose $k$ secret split as being worth $v/k$ where
$v$ represents the cost of the unsharded secret and $k$ represents the
number of shards necessary to reconstitute the secret. In such a
situation, the user's secret is effectively leaked when $k$ shards are
exposed. The exposure of $k$ shards allows the user to seek damages
for a value equal to $v/k$ from each of the $k$ SSPs responsible,
resulting in the same value $v$ the user would be due in the unsharded
case. Similarly, the cost to the user to store each secret shard
should be equal to $c/n$ where $c$ is the cost to store the unsharded
secret and $n$ is the number of total SSPs in use. As in the liability
valuation, such a split will ensure the user pays the same for a
sharded secret as an unsharded secret, at least in cases where
$k=n$. Since $k$ choose $n$ secret sharding schemes allow for the user
to specify a $k$ greater than $n$ for the purpose of adding redundancy
to the system, a user may still end up paying more to store a sharded
secret than an unsharded secret in any case where $k>n$. In such
situations, however, the user is gaining an additional degree of
redundancy that they did not have before, justifying the additional
cost. Thus, valuing secret shards at $v/k$ for the purpose of
liability and charging for their storage at a rate of $c/k$ will avoid
financial or liability related disincentives to sharding secrets
across multiple providers.

The user can do even better than this in situations where the cost to
store a secret is exponentially or geometrically, instead of linearly,
related to the declared value of the secret. E.g. it is not
inconceivable that the cost to store a secret valued at \$1000 should
be more than 10x the cost to store a secret valued at \$10. Such a
nonlinear cost vs value curve can be justified by the fact that more
valuable secrets are likely to be more desirable targets for
attackers, and thus command a higher premium to store relative to
their value. Assuming SSPs adopt such non-linear cost regimens,
sharding a secret becomes financially beneficial to the user since it
minimizes the declared value of each individual shard, reducing the
total cost. Whereas a linear cost of secret storage based on secret
value results in the same total cost to the user when sharding their
secrets across multiple providers, a non-linear secret storage cost
can cause the user to realize a savings, further encouraging the
practice. It is thus potentially desirable for SSPs to adopt
non-linear secret storage cost regimes where the cost to store a
secret increases geometrically or exponentially with its declared
value. Such an ecosystem would further encourage users to shard their
secrets across multiple providers, increasing user security while
reducing both SSP trust and cost.

\subsubsection{Jurisdictional Issues}
\label{chap:policy:trust:jurisdiction}

The final policy component implicit to the sharding of secrets across
multiple SSPs is that of the multi-jurisdictional issues such sharding
might raise. If a user limits their sharding of secrets to SSPs
located within a single regulatory domain, jurisdictional issues are
moot. But it is possible (and, as mentioned previously, desirable) for
a user to shard their secret across a range of multi-jurisdictional
SSPs. Such sharding both helps to insulate the user against privacy
failures implicit to any single jurisdiction (e.g. spying by
oppressive governments) and ensures the user has access to the widest
possible diversity of SSPs, increasing the likelihood that market
forces can help maximize SSP trustworthiness.

How regulators might view the sharding of secrets across multiple
domains is debatable. On one hand, such sharding is likely to subvert
legitimate efforts by officials to access stored user data (for
example, access granted via a probable cause warrant) due to the need
to enforce such access requests across multiple jurisdictions. This is
likely to frustrate regulators. On the other hand, multi-national
secret sharding has clear benefits for end user security and the
availability of user data by minimizing the kind of privacy failure
risks that tend to be jurisdiction septic (e.g.  the overthrow of a
legitimate government by an illegitimate one). Such challenges are not
necessarily unique to multi-jurisdictional secret sharding. Activities
such as Internet-based gambling have long leveraged conflicts between
varying jurisdictional laws to operate in a legal gray
area~\cite{miller2006}. Additional jurisdictional arbitrage examples
range from seasteading~\cite{balloun2012} to corporate
law~\cite{kocaoglu2008}.

Law enforcement organizations (LEOs) wishing to legally reconstitute a
multinationally sharded secret face an uphill battle. First, such
actors would have to employ one of the variety of mechanisms designed
to aid law enforcement in recovering evidence across national
borders. Such mechanism often consists of mutual legal assistance
treaties (MLATs) laying out a framework for how LEOs in one country
can request the assistance of LEOs in another
country~\cite{stigall2013, walden2011}. Such efforts, however, tend to
be slow and burdensome. Multiply such effort by the number of shards a
user might have, each stored with an SSP in a different country, and
it becomes increasingly difficult to legally obtain the necessary
shards to reconstitute a secret~\cite{kent2015}. Furthermore, there
are some countries that may lack MLATs all together, or that lack the
kind of centralized government and rule of law necessary to make an
MLAT enforceable. Thus sharding secrets across multiple multinational
SSPs imposes a significant burden upon anyone wishing to compel such
SSPs to provide their shards of the secret.

While this arrangement may appear as a bug to those wishing to enforce
lawful data requests, it is probably a feature to those wishing to
avoid such requests. Indeed, and as mentioned in
Chapter~\ref{chap:trust}, it is likely that certain SSPs would
specifically market their resistance to compelled legal orders as a
reason why users should select them as an SSP. In fact, SSPs might
even encourage a form of international regulatory competition where
countries strive to pass more stringent privacy protects in order to
attract SSPs and the business they might generate. Such an outcome is
likely good for users on the whole since competition between
governments is likely to lead to better privacy-related laws for all
users. And given the range of legal abuses practiced by governments
around the world, the ability to leverage a multinational network of
SSPs to gain more favorable privacy protections is a desirable
capability.

But regardless of benefit, it is likely that the multinational SSP
sharding will be used to subvert certain wholly legitimate
investigations. One way to counter this might be pursue efforts to
streamline and expedite existing MLAT
processes~\cite{nojeim2015}. Indeed, given the automated nature of the
SSP ecosystem, it is likely that certain MLAT-related functions could
be automated for the benefit of those placing lawful data
requests. But even with such efforts, it is unlikely that all SSPs
will ever be subject to a single set of legal obligations and
requirements, ensuring that some degree of jurisdictional arbitrage
always remains possible. These issues are not unique to the SSaaS
ecosystem. They effect a wide range of digital services and thus
likely demand a more general solution than the SSaaS ecosystem itself
can provide. It is useful to provide LEOs with satisfactory methods
for securing lawful access to user data across borders, lest they
become discouraged and instead seek to implement detrimental data
localization laws that ban the storage of data aboard all
together~\cite{whitehouse2013}. But as in the cryptography discussion,
any effort to grant lawful access to sharded data that reduces a
user's ability to shard their data in the first place is likely to
cause more harm than good. For every criminal sharding protects, there
will be a multitude of everyday users, dissidents, and other good
faith actors that the inability to leverage sharding would harm.

Multinational SSP sharding thus represents yet another level of trust
mitigation. Sharding secrets between SSPs allows the user to reduce
trust in any single SSP. Sharding secrets between nations allows the
user to reduce trust in any single nation. And given many nations'
penchant for privacy abuses, that is a useful trust mitigation to
have.

%%  LocalWords:  SSaaS DES PGP's backdoored Slipjack Snowden NSA LEOs
%%  LocalWords:  Wassenaar nd th Tutamen CAs OpenSSL SSP SSPs Todo
%%  LocalWords:  ECPA SSP's Custos DIY GitHub Shamir's Balkanisation
%%  LocalWords:  seasteading MLATs MLAT passcode
